image channels: 

CH0 = G
CH1 = B
CH2 = NIR
CH3 = R


plantcv requires background image
don't think plantcv is necessary, trying NDVI

NDVI 
(NIR - R)/(NIR + R)

NDVI doesn't really work

trying blue filter, works ok. it misses some stuff but does ok

experiment changes at 13_39_22, either image or experiment rotates and the triangle is erased
emailed rachel

rgb image looks kinda garbage, not sure why. tried all the r,g,b orders and r,g,b looks most normal


exclude 8 frames prior to stress time (8_57)


koppal says blue filter looks good


labels csv creator is done
now setup dataloader


3/3/21
change csv / dataloader layout: 

index   img_name        time    stress  channel
0       ...ch0.tiff     N       M       G
1       ...ch1.tiff     N       M       B
2       ...ch2.tiff     N       M       NIR
3       ...ch3.tiff     N       M       R
4       ...ch0.tiff     N+1     M+1     G
5       ...ch1.tiff     N+1     M+1     B
...     ...             ...     ...     ...

index   img_name        time    stress  channel

0       [...ch0.tiff,   N       M       [G,
         ...ch1.tiff,                    B,
         ...ch2.tiff,                    NIR,
         ...ch3.tiff]                    R]

1       ...             N+1     M+1     ...


3/8/21
finished updating labeling layout from 3/3
now test dataloader class. want to ask for one sample, returns 4 tiffs, array of channels, 
time image was taken wrt experiment start, and time since stress

4/3/21
dataloader outputs goodly, need to test applying a mask? 

formatting question: should a binary mask matrix be passed to the Mask class or should it be generated automatically inside the class? 

3:21- idk how it happened but it looks like my work to construct the image part of the sample dict is gone. might have a copy on my home machine. :((
this is the kick in the pants to bootstrap git for this project. idk why i didnt at the jump. just emailed koppal about whether focus lab has a git. 

8:19- fixed it. had a backup on home machine. blue mask is working. next step is masking control quadrant. i think best soln is two mask classes: b_mask and ctrl_mask or the like. then compose both. maybe theres a different way to handle the control masking. not sure. 
maybe the control quadrant needs separate entries in the dataset.  

4/4/21:
goal: get everything on github and implement control mask class

4/15/21: 
goal: implement dataloader idea with following format: 

top layer: N experiment [0:3], this is each of the plates of the experiment. treating each plate as it's own experiment. 
after selecting an experiment you get the previous format. 
{images: [mxnx4 nparray], time since experiment start: int, time since stress: int: channel array: [G,B,NIR,R]}

this is really just adding an additional layer st instead of getting the raw images you get the image of the plate. 

4/16/21: 
fixed a small bug with the blue_threshold 
tested splitting image strictly along 4 quadrants. works on one test image but I should try a batch to make sure it works throughout experiment. 

4/18/21: 
idea: if we are treating each slide as an experiment, what if when conducting the experiment they are stressed at different times. ie plate 1 @ 30 mins, plate 2 @ 1 hr, plate 3 @ 1.5 or 2 hrs. idk if that would help reduce algorithm dependence on stress time vs experiment time 

fixed match case syntax error, replaced with if/elif chain. may be problem with mask erasing entire img. more testing needed


5/3/21: 
installed project on focuslab cluster
densenet time? 

6/4/21: 
fixed git issue, everything is current now. 


6/21/21: 

subsampling dataset using idx[0] and idx[1] isn't going to work, idx is generated automatically by pytorch. 
for now: lock to experiment 0 (top left)
in future: process images into 4 different folders? actually split up the images and categorize them. 
not sure overall how to make the rnn process those folders though. to be figured out later. 

6/23/21: 
how to maintain unique values but turn 4 channels per pixel into one value per pixel
brevin says to use convrnn
(t/s,b,c,h,w): (time/sequence length, batch size, channels, height, width)

6/29/21: 

got the image input in the right shape and everything is flowing. need to change 
the shape of the labels i think to match the input. it is currently ([b,t]) and i think should be (b, hidden_layer, h, w). at least that's what pytorch is asking for: 
"ValueError: Expected target size (5, 64, 256, 256), got torch.Size([5, 10])"
not sure. 

6/30/21: 
in my own head about how the labels / output layer should be structured. going to try to get this working as a binary classifier, stressed or not stressed. 

7/02/21: 
met with sanjeev today. he said not to touch anything else and focus on testing the current implementation. 
concern: the network is learning the mask and will make it not work on other experiments/quadrants. but maybe not. maybe starting with pretrained weights from one experiment will reduce training time for other applications
try: training for different lengths of time (different # of epochs?)
implement: separate test/train datasets, testing quantification. 
question: is an out of 0.95 equally as wrong as a 0.65 when the gt is 1? i dont think the network spits out a perfect 1 

epoch tests: 1, 3, 5, 10, 20 epochs

hyperparameters: 
sequence_length = 10
input_dim = 4
hidden_dim = [128,64,8,1]
num_layers = 4
output_dim = 1


tried to squish with sgmoid. its weird and never outputting a 0, more fiddling needed. 


7/06/21: 

trying to implement weighted loss s/t it counteracts the imbalanced dataset

current problem is the input to crossentropyloss


7/07/21: 
ok got it working 

epoch tests: 1, 3, 5, 10, 20 epochs 

hyperparameters: 
sequence_length = 10
input_dim = 4
hidden_dim = [128, 64, 8, 1]
num_layers = 4
output_dim = 1
batch_szie = 1
num_epochs = x
learning_rate = 0.01
kernel_size = (3,3)

results:: 
epoch   test_loss   accuracy    model_name
    1   32.98       55          cel_model_1 
    3   75          27          cel_model_3
    5   32.98       27.5        cel_model_5
   10   75.98       27.5        cel_model_10
   20   32.98       27.5        cel_model_20
  100   32.98       27.5        cel_model_100


07/09/21: 

going to write a script to run each epoch length and output the results to a file
that way it'll be easier to adjust hyperparameters and then test all epoch lengths. 

lr test: 0.005, 0.001, 0.0005, 0.0001
batch_size: 1

epoch:  1               3               5               10              20              100
lr      tl      acc     tl      acc     tl      acc     tl      acc     tl      acc     tl      acc
0.005   32.98   27.5    32.98   27.5    32.98   27.5    75.98   27.5    75.98   27.5    32.98   27.5
0.001   32.98   27.5    32.98   27.5    32.98   27.5    32.98   27.5    32.98   27.5    32.98   27.5
0.0005  32.98   27.5    32.98   27.5    32.98   27.5    75.98   27.5    32.98   27.5    32.98   27.5
0.0001  32.98   27.5    32.98   0       23.98   0       32.98   27.5    32.98   27.5    32.98   27.5


07/12/21: 
above run has finished and everything is still 32.98/27.5 
not sure if that means that the model is reaching peak performance or if its not actually working 
there are a few anomalies. need sanjeev to help me interpret I think. also need to learn more nitty about how the loss works. 
i know lower loss = better, but how is it actually calculated? 

07/14/21: 
talked with sanjeev yesterday, need to try lower learning rate and look at other binary classification architectures

should i also try standardizing the input? also changing optimizer to stochastic gradient descent from 'Adam' 

07/20/21: 
testing adding more linear layers for reduction and adding a relu activation layer between each. 
testing 4 reduction layers. so going from h*w = 256**2, [h*w -> 1 -> sigmoid] to 
[h*w -(relu)-> h*w/2 -(relu)-> h*w/4 -(relu)-> h*w/8 -(relu)-> 1 -> sigmoid] 
i worry this is too much addition, if doesn't work try simplifying to maybe 2 layers? 
[h*w -(relu)-> h*w/2 -(relu)-> 1 -> sigmoid]

running a test at following parameters: 

sequence_length = 10 
input_dim = 4                   # number of image channels
hidden_dim = [128, 64, 8, 1]    # hidden layer dimensions
num_layers = 4                  # number of hidden layers
output_dim = 1                  # size of linear output layer
batch_size = 1                  # number of samples per batch
num_epochs = 5                  # loop over entire dataset this many times
learning_rate = 0.00001            # learning rate for gradient descent
kernel_size = (3,3)             # kernel size for convolution layer

this is also lowering the lr by 1 order of mag 

i also worry that adding this much stuff is going to decrease performance mega
update: after trying it runs out of memory so fast, even on batch size 1. need to reduce that load. 
reconfigured the layers s/t each layer overwrites the last, so each isn't kept in memory

still died, ran out of mem @ back prop,line 130: loss.backward()   


07/23/21: 

planning block diagram: 
image -> mask -> convlstm -> linear + activation function layers -> binary classification 


07/26/21: 
met with plant lab on friday. they said it was unlikely that there could be any change seen over a 10 minute window. 
try expanding to 40 mins. [t-9, t-8, ..., t-1, t] --> [t-40, t-36, t-32, t-28, t-24, t-20, t-16, t-12, t-8, t-4, t]
can also mess around with distribution, maybe [t-40, t-30, t-20, t-15, t-10, t-5, t] ?? dunno 
going to roll back activation layers thing so i can get some memory back.

implementing adjustable sequence length in the following way: 
seq_length remains the same. it's the length of the time series array, 
    ie: seq_length = 10 --> [ -9,  -8,  -7,  -6,  -5,  -4,  -3, -2, -1, 0] or 
                            [-36, -32, -28, -24, -20, -16, -12, -8, -4, 0] 

seq_range is the range of the time series array, 
    ie: for the above, seq_range = 10 for the top and seq_range = 40 for the bottom

this array is constructed using torch.arange(start, stop, step)
for seq_length = 10, seq_range = 40 which aims for the following array: 
[-36, -32, -28, -24, -20, -16, -12, -8, -4, 0] 
you use 
torch.arange(sample_id-(seq_range-(seq_range/seq_length)), sample_id+(seq_range/seq_length), seq_range/seq_length)


07/27/21: 

first results from epoch=5, lr=0.00001, seq_length = 10, seq_range = 40: (cel_model_5_series.ckpt)

test_loss: 46.21
accuracy: 56/67 ( correct stress predictions: 56/56, correct unstressed predictions: 0/11 )

one note is all the outputs of the network were very similar but not exactly the same.
They all looked like [0.4952, 0.5048] with each being +/- 0.001

trying some with more epochs

epoch=10: (cel_model_10_series.ckpt)

same overall reults: 
test_loss:  43.64762884378433
accuracy: 56/67
correct stress predictions: 56/56
correct unstressed predictions: 0/11

however the outs were more polarized: [0.4350, 0.5650] still +/- 0.001


trying higher lr now (cel_model_10_series_lr0.0001.ckpt)

same overall results, outs were more polarized [0.038, 0.962] but always the same output pretty much 

sent koppal an update and he said to tackle fixing the stuck output thing first. 
trying to increase hidden layers now. 

07/28/21: 

running out of memory if i increase hidden layer size even a little. 
going to try decreasing seq_length, maybe 10 -> 4? with same range? -> [t-36, t-24, t-12, t]

10/01/21: 
long time no type my friend 

simplifying network time. switching to a cnn using prebuilt pytorch modules. 

current data shape is [batch, depth (time), channels, height, width] 
for conv3d need       [batch, channels, depth, height, width] 

hard to visualize, not sure how to make that change

actually going to try keeping the same, so time is treated as the channels, and depth is the 4 spectra 
keep [b,t,c,h,w]

it runs forward

it runs all the way thru, testing broken atm. 


new date style: 
09/11/21: (nov)
trying to solve issue of oversampling low index images. this is fixing the bandaid of pushing low index values forward if they attempt to grab negative index images. 
doing this by creating a subsample depending on shape of time-series. then using that dataset as the target of the enumerate call in the main loop 
however, it doesn't get used beyond that, the original dataset gets sampled. 
need to see if sample index changes. ie i want to sample dataset 1 which goes from 0-300, however the 
time series is shaped like t = [0, 10, 20, 30] so i only want to sample 30-300, 
if i create a subset which is 30-300, does it stay t = 30-300 or change to 0-270. 
if the latter is the case, i can push it forward but would prefer not to.  

^ this is the case, need to push forward by: sequence_range-(sequence_range/sequence_length)
also need to write an exception s/t a range/length can't be selected which is not ints

got this partly implemented, todo: test/train split, implement weighted sampling and then implement in loop. 

11/11/21: 
this is gonna pitch a fit when batch sizes come into play. 

todo: weightedrandomsampler ---> data augmentation

11/14/21: 
weighted random sampler is harder to implement than i thought. status: have a tensor of weights. not sure how to squish that with the train/test split. 

11/26/21: 
need to make a transform class which deletes random percentage of pixels in the image 

03/12/21: 
implemented the random deletion filter. with p=0.5, got all stressed predictions right with like [0.15,0.85]ish values. got all unstressed wrong with like [0.35,0.65] which is better than it used to be. going to try p=0.8 and see what changes

p=0.8 improved the confidence of both the stressed guess [0.1,0.9] and a little bit of the unstressed guesses, [0.4,0.6]
going to try messing with the weighting again. maybe [0.6,0.4]? not much change 
now going back to default weights but with p=1, should be the same as black frame vs plant frame. 
it works, but confidence is still bad for unstressed, [0.55,0.45] ish. and for stressed, [0.001, 0.999] 
not sure why this disparity exists. maybe need to train longer, 50 epochs? 

13/12/21: 

writing up a report for the end of the semester and had an idea, instead of darkening unstressed images, maybe brightening the stressed images would be better. makes it a comparison between normal unstressed images and stressed images ranging from normal (~10) to white (255)
also maybe instead of setting random pixels to 0, set them to 255. bigger difference between that and the background
